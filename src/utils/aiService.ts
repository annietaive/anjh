/**
 * AI Service - T√≠ch h·ª£p Gemini AI th√¥ng minh cho Teacher Emma
 * 
 * üéØ 3 CHI·∫æN L∆Ø·ª¢C HO·∫†T ƒê·ªòNG (Auto fallback):
 * 1. Gemini Direct API (Client-side) - Nhanh nh·∫•t, c·∫ßn VITE_GEMINI_API_KEY
 * 2. Backend Server Proxy - B·∫£o m·∫≠t h∆°n, c·∫ßn VITE_SERVER_URL
 * 3. Built-in AI Pattern Matching - Fallback lu√¥n ho·∫°t ƒë·ªông
 * 
 * üìù SETUP GEMINI API (1 ph√∫t):
 * 1. L·∫•y FREE API key: https://makersuite.google.com/app/apikey
 * 2. T·∫°o file .env: VITE_GEMINI_API_KEY=your_key_here
 * 3. Restart server: npm run dev
 * 
 * ‚úÖ Xem chi ti·∫øt: /GEMINI_SETUP.md
 */

import { getSupabaseClient } from './supabase/client';
import { getEnv } from './env';

export interface AIResponse {
  content: string;
  similarQuestion?: string;
  answer?: string;
  confidence?: number;
}

// ============================================================================
// MEMORY & CONTEXT MANAGEMENT
// ============================================================================

class ConversationMemory {
  private context: string[] = [];
  private maxContextLength = 5;

  addMessage(role: 'user' | 'assistant', content: string) {
    this.context.push(`${role}: ${content}`);
    if (this.context.length > this.maxContextLength * 2) {
      this.context = this.context.slice(-this.maxContextLength * 2);
    }
  }

  getContext(): string {
    return this.context.join('\n');
  }

  clear() {
    this.context = [];
  }
}

const memory = new ConversationMemory();

// ============================================================================
// BUILT-IN AI LOGIC (FALLBACK)
// ============================================================================

interface ResponsePattern {
  keywords: string[];
  responses: string[];
  confidence: number;
}

const responsePatterns: ResponsePattern[] = [
  // Math patterns
  {
    keywords: ['c·ªông', 'add', 'plus', '+', 't·ªïng'],
    responses: [
      'ƒê·ªÉ gi·∫£i b√†i to√°n c·ªông, em c·∫ßn c·ªông c√°c s·ªë l·∫°i v·ªõi nhau. V√≠ d·ª•: 5 + 3 = 8',
      'Ph√©p c·ªông l√† ph√©p t√≠nh ƒë∆°n gi·∫£n nh·∫•t! Em h√£y th·ª≠ c·ªông t·ª´ng s·ªë m·ªôt nh√©.',
      'C√°ch l√†m: L·∫•y s·ªë th·ª© nh·∫•t, c·ªông th√™m s·ªë th·ª© hai. Th·ª≠ l√†m v√≠ d·ª• xem!'
    ],
    confidence: 0.8
  },
  {
    keywords: ['tr·ª´', 'subtract', 'minus', '-', 'hi·ªáu'],
    responses: [
      'Ph√©p tr·ª´ l√† l·∫•y s·ªë l·ªõn tr·ª´ ƒëi s·ªë nh·ªè. V√≠ d·ª•: 10 - 4 = 6',
      'ƒê·ªÉ t√≠nh hi·ªáu, em l·∫•y s·ªë b·ªã tr·ª´ tr·ª´ ƒëi s·ªë tr·ª´ nh√©!',
      'C√°ch nh·ªõ: S·ªë tr∆∞·ªõc d·∫•u "-" l√† s·ªë b·ªã tr·ª´, s·ªë sau l√† s·ªë tr·ª´.'
    ],
    confidence: 0.8
  },
  {
    keywords: ['nh√¢n', 'multiply', 'times', '√ó', '*', 't√≠ch'],
    responses: [
      'Ph√©p nhn l√† c·ªông l·∫∑p l·∫°i nhi·ªÅu l·∫ßn. V√≠ d·ª•: 3 √ó 4 = 3 + 3 + 3 + 3 = 12',
      'ƒê·ªÉ nh√¢n nhanh, em c√≥ th·ªÉ h·ªçc thu·ªôc b·∫£ng c·ª≠u ch∆∞∆°ng nh√©!',
      'M·∫πo: 3 √ó 4 nghƒ©a l√† "3 ƒë∆∞·ª£c l·∫•y 4 l·∫ßn" ho·∫∑c "4 ƒë∆∞·ª£c l·∫•y 3 l·∫ßn"'
    ],
    confidence: 0.8
  },
  {
    keywords: ['chia', 'divide', '√∑', '/', 'th∆∞∆°ng'],
    responses: [
      'Ph√©p chia l√† chia ƒë·ªÅu m·ªôt s·ªë ra nhi·ªÅu ph·∫ßn. V√≠ d·ª•: 12 √∑ 3 = 4',
      'C√°ch l√†m: H·ªèi s·ªë l·ªõn ch·ª©a bao nhi√™u l·∫ßn s·ªë nh·ªè.',
      'M·∫πo: Ph√©p chia l√† ph√©p ngh·ªãch ƒë·∫£o c·ªßa ph√©p nh√¢n!'
    ],
    confidence: 0.8
  },

  // English Grammar patterns  
  {
    keywords: ['present simple', 'hi·ªán t·∫°i ƒë∆°n', 'th√¨ hi·ªán t·∫°i'],
    responses: [
      'Th√¨ hi·ªán t·∫°i ƒë∆°n d√πng ƒë·ªÉ di·ªÖn t·∫£:\n‚úÖ S·ª± th·∫≠t hi·ªÉn nhi√™n: The sun rises in the east.\n‚úÖ Th√≥i quen: I go to school every day.\n‚úÖ S·ªü th√≠ch: She likes music.\n\n**C√¥ng th·ª©c**: S + V(s/es) + O',
      'Present Simple c√≥ 3 d·∫°ng:\n‚Ä¢ Kh·∫≥ng ƒë·ªãnh: I/You/We/They + V, He/She/It + V(s/es)\n‚Ä¢ Ph·ªß ƒë·ªãnh: S + do/does not + V\n‚Ä¢ Nghi v·∫•n: Do/Does + S + V?',
      'D·∫•u hi·ªáu nh·∫≠n bi·∫øt: always, usually, often, sometimes, never, every day/week/month'
    ],
    confidence: 0.9
  },
  {
    keywords: ['present continuous', 'hi·ªán t·∫°i ti·∫øp di·ªÖn', 'ƒëang x·∫£y ra'],
    responses: [
      'Th√¨ hi·ªán t·∫°i ti·∫øp di·ªÖn d√πng ƒë·ªÉ di·ªÖn t·∫£ h√†nh ƒë·ªông ƒëang x·∫£y ra t·∫°i th·ªùi ƒëi·ªÉm n√≥i.\n\n**C√¥ng th·ª©c**: S + am/is/are + V-ing\n\nV√≠ d·ª•: I am studying English now.',
      'C√°ch th√™m -ing:\n‚Ä¢ Th∆∞·ªùng: play ‚Üí playing\n‚Ä¢ T·∫≠n c√πng -e: make ‚Üí making\n‚Ä¢ 1 nguy√™n √¢m + 1 ph·ª• √¢m: run ‚Üí running',
      'D·∫•u hi·ªáu: now, at the moment, at present, right now, Look!, Listen!'
    ],
    confidence: 0.9
  },
  {
    keywords: ['past simple', 'qu√° kh·ª© ƒë∆°n', 'ƒë√£ x·∫£y ra'],
    responses: [
      'Th√¨ qu√° kh·ª© ƒë∆°n d√πng ƒë·ªÉ di·ªÖn t·∫£ h√†nh ƒë·ªông ƒë√£ x·∫£y ra v√† k·∫øt th√∫c trong qu√° kh·ª©.\n\n**C√¥ng th·ª©c**: S + V-ed/V2 + O\n\nV√≠ d·ª•: I visited Ha Long Bay last summer.',
      'ƒê·ªông t·ª´ c√≥ 2 lo·∫°i:\n‚Ä¢ Quy t·∫Øc: th√™m -ed (play ‚Üí played)\n‚Ä¢ B·∫•t quy t·∫Øc: go ‚Üí went, eat ‚Üí ate, see ‚Üí saw',
      'D·∫•u hi·ªáu: yesterday, last week/month/year, ago, in + nƒÉm qu√° kh·ª©'
    ],
    confidence: 0.9
  },

  // Greetings
  {
    keywords: ['xin ch√†o', 'hello', 'hi', 'ch√†o'],
    responses: [
      'Xin ch√†o em! üëã T√¥i l√† Teacher Emma, tr·ª£ l√Ω h·ªçc t·∫≠p AI c·ªßa em. Em c·∫ßn gi√∫p g√¨ h√¥m nay?',
      'Hi em! üòä R·∫•t vui ƒë∆∞·ª£c g·∫∑p em. Em mu·ªën h·ªçc v·ªÅ ch·ªß ƒë·ªÅ g√¨ n√†o?',
      'Hello! üåü T√¥i s·∫µn s√†ng gi√∫p em h·ªçc t·∫≠p. H√£y ƒët c√¢u h·ªèi nh√©!'
    ],
    confidence: 0.9
  },

  // Thanks
  {
    keywords: ['c·∫£m ∆°n', 'thank', 'thanks', 'c√°m ∆°n'],
    responses: [
      'Kh√¥ng c√≥ chi em! üòä T√¥i lu√¥n s·∫µn s√†ng gi√∫p em h·ªçc t·∫≠p.',
      'Em th·∫≠t l·ªãch s·ª±! C·ª© h·ªèi t√¥i b·∫•t c·ª© l√∫c n√†o em c·∫ßn nh√©! üåü',
      'You\'re welcome! Ch√∫c em h·ªçc t·ªët! üí™'
    ],
    confidence: 0.9
  }
];

function generateSmartResponse(question: string, context?: string): AIResponse {
  const lowerQuestion = question.toLowerCase();
  
  // Try to find matching patterns
  for (const pattern of responsePatterns) {
    for (const keyword of pattern.keywords) {
      if (lowerQuestion.includes(keyword.toLowerCase())) {
        const randomResponse = pattern.responses[Math.floor(Math.random() * pattern.responses.length)];
        return {
          content: randomResponse,
          confidence: pattern.confidence
        };
      }
    }
  }

  // Default response if no pattern matches
  return {
    content: `Xin l·ªói em, c√¢u h·ªèi n√†y h∆°i kh√≥ v·ªõi t√¥i. ü§î

T√¥i c√≥ th·ªÉ gi√∫p em v·ªÅ:
üìö **Ti·∫øng Anh**: Grammar (6 th√¨), vocabulary, reading, writing
üî¢ **To√°n h·ªçc**: C·ªông, tr·ª´, nh√¢n, chia, ph√¢n s·ªë, h√¨nh h·ªçc
üî¨ **Khoa h·ªçc**: V·∫≠t l√Ω, h√≥a h·ªçc, sinh h·ªçc c∆° b·∫£n
üìñ **Ph∆∞∆°ng ph√°p h·ªçc t·∫≠p**: K·ªπ thu·∫≠t ghi nh·ªõ, √¥n thi hi·ªáu qu·∫£

Em h√£y th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n nh√©! V√≠ d·ª•: "Th√¨ hi·ªán t·∫°i ƒë∆°n l√† g√¨?" ho·∫∑c "L√†m th·∫ø n√†o ƒë·ªÉ h·ªçc t·ª´ v·ª±ng hi·ªáu qu·∫£?"`,
    confidence: 0.3
  };
}

// ============================================================================
// GEMINI API VIA BACKEND SERVER
// ============================================================================

async function callGeminiViaBackendServer(question: string, context?: string): Promise<string> {
  try {
    // Get server URL from environment manager
    const serverUrl = getEnv('VITE_SERVER_URL') || '';
    
    if (!serverUrl) {
      console.warn('‚ö†Ô∏è VITE_SERVER_URL not configured');
      throw new Error('Backend server URL not configured');
    }
    
    // Call backend server endpoint
    const response = await fetch(`${serverUrl}/make-server-bf8225f3/ai-chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ question, context })
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      console.error('‚ùå Backend server error:', response.status, errorData);
      throw new Error(errorData.error || `Server error: ${response.status}`);
    }

    const data = await response.json();

    if (!data?.content) {
      console.error('‚ùå Invalid response format:', data);
      throw new Error('Invalid AI response format');
    }

    console.log('‚úÖ Gemini AI success (via Backend Server)');
    return data.content;

  } catch (error) {
    console.error('‚ùå Gemini Backend Server call failed:', error);
    throw error;
  }
}

// ============================================================================
// GEMINI API - DIRECT CALL (Client-side with API Key)
// ============================================================================

async function callGeminiDirectly(question: string, context?: string): Promise<string> {
  try {
    // Get API key from environment manager (will use hardcoded fallback)
    const apiKey = getEnv('VITE_GEMINI_API_KEY');
    
    if (!apiKey) {
      console.warn('‚ö†Ô∏è VITE_GEMINI_API_KEY not found');
      throw new Error('Gemini API key not configured');
    }
    
    console.log('‚úÖ API Key loaded successfully');

    // System prompt for Teacher Emma
    const systemPrompt = `You are Teacher Emma, a friendly and encouraging AI teacher for Vietnamese middle school students (grades 6-9). 

Your capabilities:
‚ú® Explain English grammar, vocabulary clearly in Vietnamese
üìö Help with Math, Science, and other subjects
‚úçÔ∏è Check writing and provide detailed feedback
üí¨ Chat naturally and remember conversation context
üéØ Adapt explanations to student's level

Guidelines:
- Be patient, supportive, and encouraging
- Use simple language appropriate for middle school
- Provide examples and practice exercises
- Use emojis to make learning fun üòä
- Respond in Vietnamese unless asked in English
- Remember previous messages in the conversation
- Keep responses concise but informative (under 500 words)
- Break down complex topics into simple steps

${context ? `Previous conversation context:\n${context}\n\n` : ''}Student's question: ${question}`;

    // Use gemini-flash-latest as requested
    const models = [
      'gemini-flash-latest'
    ];

    for (const model of models) {
      try {
        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: [{
                  text: systemPrompt
                }]
              }],
              generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 2000,
                topP: 0.95,
                topK: 40,
              },
              safetySettings: [
                {
                  category: "HARM_CATEGORY_HARASSMENT",
                  threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                  category: "HARM_CATEGORY_HATE_SPEECH",
                  threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                  category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                  threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                  category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                  threshold: "BLOCK_MEDIUM_AND_ABOVE"
                }
              ]
            })
          }
        );

        if (!response.ok) {
          const errorText = await response.text();
          
          // Parse error to check if it's quota error (429)
          let isQuotaError = false;
          try {
            const errorData = JSON.parse(errorText);
            if (errorData.error?.code === 429 || errorData.error?.status === 'RESOURCE_EXHAUSTED') {
              isQuotaError = true;
              console.warn(`‚ö†Ô∏è Model ${model} quota exceeded, trying next model...`);
            } else {
              console.warn(`‚ö†Ô∏è Model ${model} error ${response.status}, trying next...`);
            }
          } catch (e) {
            console.warn(`‚ö†Ô∏è Model ${model} failed with status ${response.status}, trying next...`);
          }
          
          continue; // Try next model
        }

        const data = await response.json();
        const aiText = data.candidates?.[0]?.content?.parts?.[0]?.text;

        if (!aiText) {
          console.error(`‚ùå Invalid response from ${model}:`, data);
          continue; // Try next model
        }

        console.log(`‚úÖ Gemini AI success with model: ${model}`);
        return aiText;

      } catch (modelError) {
        console.error(`‚ùå Error with model ${model}:`, modelError);
        continue; // Try next model
      }
    }

    throw new Error('All Gemini models failed to respond');

  } catch (error) {
    console.error('‚ùå Gemini Direct API call failed:', error);
    throw error;
  }
}

// ============================================================================
// MAIN AI SERVICE - WITH AUTO FALLBACK
// ============================================================================

export async function getAIResponse(question: string): Promise<AIResponse> {
  let response: AIResponse;
  const context = memory.getContext();

  try {
    // Strategy 1: Try Gemini API directly (fastest, client-side with default API key)
    console.log('üöÄ Teacher Emma ƒëang suy nghƒ©...');
    const content = await callGeminiDirectly(question, context);
    response = { content, confidence: 0.95 };

    // Store in memory for context
    memory.addMessage('user', question);
    memory.addMessage('assistant', response.content);

    return response;

  } catch (error: any) {
    console.log('üí≠ Gemini API kh√¥ng kh·∫£ d·ª•ng, ƒëang chuy·ªÉn sang Built-in AI...');
    
    // Strategy 2: Fallback to built-in AI (always works)
    // Note: Backend server strategy skipped due to deployment restrictions
    response = generateSmartResponse(question, context);
    
    memory.addMessage('user', question);
    memory.addMessage('assistant', response.content);
    
    return response;
  }
}

/**
 * Clear conversation memory
 */
export function clearMemory() {
  memory.clear();
}